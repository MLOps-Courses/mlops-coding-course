# 0.3. Platforms

## What is an MLOps platform?

An MLOps platform integrates various infrastructure components necessary for deploying and managing AI/ML projects in a production environment. These platforms typically encompass:

- Storage Systems (e.g., S3, cloud storage)
- Compute Engines (e.g., Kubernetes, Databricks)
- Orchestrators (e.g., Airflow, Metaflow, Prefect)
- Model Registries (e.g., MLflow, Neptune.ai, Weights and Biases)

The size and complexity of an MLOps platform can vary based on organizational needs and use cases. Smaller organizations might lean towards open-source solutions like MLflow and Airflow, whereas larger enterprises may opt for comprehensive platforms like Databricks or SageMaker to facilitate AI/ML projects at scale.

## Which MLOps platform is the best?

Selecting the "best" MLOps platform is subjective and highly dependent on specific organizational needs and available expertise. For example, a company not utilizing Kubernetes might find it challenging to adopt a Kubernetes-centric platform for AI/ML services. Conversely, organizations comfortable with Kubernetes may prefer it to circumvent the costs associated with managed services like SageMaker or Vertex AI.

Here are some recommendations for selecting an MLOps platform:

1. Engage with stakeholders within your organization, including data scientists, operations, and architects, to understand requirements.
1. Assess your objectives and the maturity level needed to achieve your goals within the desired timeline.
1. Conduct pilot projects to evaluate whether the chosen solution aligns with your business needs and user expectations.

## Why you did not choose an MLOps platform?

Vendors in the market often promote their MLOps platforms and ease their adoption through training and certification. However, these promotions typically showcase simplified projects, which may not address the complexities of developing robust AI/ML codebases adhering to established software principles.

Although each platform offers a unique developer experience, the fundamental skills in coding for MLOps are versatile and apply broadly across various AI/ML initiatives.

## Does this course require an MLOps platform?

This course is designed to be platform-agnostic and does not necessitate the use of a specific MLOps platform. Participants are encouraged to tailor the course content to fit the technological landscape of their organizations. This may include incorporating elements like environment management (e.g., development, staging, pre-production) and additional libraries or aligning it with existing organizational practices.

For example, if an organization prefers using internal tools over GitHub, such as GitLab or Azure DevOps, the course content can be adapted accordingly, ensuring that each chapter is relevant and applicable within your specific context.

## How this course will be beneficial for using my MLOps platform?

While MLOps platforms support a wide array of software artifacts, from notebooks to scripts and Python packages, leveraging Python packages is often the most maintainable and robust approach. This course will guide you in integrating these packages with the broader Python ecosystem (e.g., pytest, coverage) and managing them efficiently via software repositories (e.g., PyPI, AWS CodeArtifact, Docker Hub).

Given the inherent complexity of MLOps projects, this course emphasizes developing high-quality, bug-free code, enabling you to concentrate on other critical aspects of your projects, such as data and model management, as well as orchestrator execution.
