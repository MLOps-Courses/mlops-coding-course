---
description: Explore the MLOps landscape and learn how to choose the right platform for your needs. This course focuses on fundamental principles, making your skills transferable across various MLOps platforms.
---

# 0.3. Platforms

## What is an MLOps platform?

An [MLOps platform](https://aws.amazon.com/what-is/mlops/) is an integrated system designed to streamline the lifecycle of machine learning models, from development and deployment to monitoring and maintenance. It provides the essential infrastructure and tooling required to manage AI/ML projects in production environments efficiently.

<figure markdown="span">
  ![CI/CD and automated ML pipeline.](https://cloud.google.com/static/architecture/images/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning-4-ml-automation-ci-cd.svg)
  <figcaption>CI/CD and automated ML pipeline (<a href="https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#mlops_level_2_cicd_pipeline_automation">source</a>).</figcaption>
</figure>

Core capabilities of an MLOps platform typically include:

- **Data and Artifact Storage**: Secure and scalable systems like [Amazon S3](https://aws.amazon.com/s3/) or [Google Cloud Storage](https://cloud.google.com/storage) for managing datasets, models, and other versioned artifacts.
- **Compute Resources**: On-demand access to computational power, such as [Kubernetes](https://kubernetes.io/) clusters or managed services like [Databricks](https://databricks.com), for model training and inference.
- **Workflow Orchestration**: Tools like [Apache Airflow](https://airflow.apache.org/), [Metaflow](https://metaflow.org/), or [Prefect](https://www.prefect.io/) that automate and manage the complex workflows and data pipelines involved in ML projects.
- **Model Registries and Experiment Tracking**: Centralized platforms like [MLflow](https://mlflow.org/), [Neptune.ai](https://neptune.ai/), or [Weights & Biases](https://wandb.ai/site) for versioning models, tracking experiment parameters, and comparing results.

The choice between open-source tools and managed enterprise solutions depends on trade-offs between flexibility, cost, and operational overhead. Smaller teams might combine tools like [MLflow](https://mlflow.org/) and [Airflow](https://airflow.apache.org/) for a custom, low-cost stack, while large organizations often prefer comprehensive platforms like [Databricks](https://databricks.com) or [AWS SageMaker](https://aws.amazon.com/sagemaker/) for their scalability and support.

## How do I choose the right MLOps platform?

Selecting the "best" MLOps platform is less about finding a single definitive answer and more about conducting a strategic evaluation of your organization's specific needs. The ideal platform is one that integrates seamlessly with your existing infrastructure, aligns with your team's technical expertise, and effectively supports your AI/ML workflows.

Follow these steps to make an informed decision:

1.  **Define Operational Needs**: Collaborate with data scientists, IT operators, and software architects to create a comprehensive list of technical and business requirements.
2.  **Align on Goals and Complexity**: Clearly define your project objectives and determine the level of platform sophistication required to meet them. Avoid over-engineering by choosing a solution that matches your current needs but can scale for the future.
3.  **Conduct Pilot Projects**: Before committing, run pilot projects on shortlisted platforms. This allows you to test the platformâ€™s capabilities against your real-world use cases and assess its usability from the end-user's perspective.
4.  **Consider Total Cost of Ownership (TCO)**: Evaluate not just the licensing fees but also the costs associated with infrastructure, maintenance, and team training.

The final decision will be influenced by your budget, your team's familiarity with certain technologies (e.g., Kubernetes), and whether you prioritize the flexibility of a custom stack or the convenience of a fully-managed service.

## What are the main categories of MLOps platforms?

MLOps platforms can be grouped into three main categories, each with distinct advantages and trade-offs:

1.  **Cloud-Provider Native Platforms**: These are services offered by major cloud providers, such as [AWS SageMaker](https://aws.amazon.com/sagemaker/), [Google Vertex AI](https://cloud.google.com/vertex-ai), and [Azure Machine Learning](https://azure.microsoft.com/en-us/products/machine-learning). They offer deep integration with their respective cloud ecosystems, providing a seamless experience for storage, compute, and other cloud services. The primary drawback is the risk of vendor lock-in.
2.  **End-to-End Commercial Platforms**: These are specialized, third-party platforms like [Databricks](https://databricks.com) and [Iguazio (now part of McKinsey)](https://www.iguazio.com/) that aim to provide a unified, managed solution covering the entire ML lifecycle. They offer a polished user experience and dedicated support but can be more expensive and may offer less flexibility than a custom-built stack.
3.  **Open-Source Stacks**: This approach involves combining various open-source tools to create a custom platform. A popular combination includes [MLflow](https://mlflow.org/) for experiment tracking, [Airflow](https://airflow.apache.org/) for orchestration, and [Kubernetes](https://kubernetes.io/) for compute. This option offers maximum flexibility and avoids licensing costs but requires significant engineering effort to build and maintain.

## Why is this course platform-agnostic?

The MLOps landscape is dynamic, with platforms and tools evolving rapidly. While vendors often emphasize the simplicity of their solutions, they can obscure the underlying engineering principles required to build robust and maintainable AI/ML systems.

This course focuses on foundational MLOps coding skills because these principles are universal and durable. Platform-specific knowledge may become outdated, but a strong grasp of software engineering best practices will remain valuable regardless of the tools you use. By mastering the fundamentals, you gain the ability to adapt to any platform and build high-quality solutions that stand the test of time.

## Is an MLOps platform required for this course?

No, this course is intentionally designed to be independent of any single MLOps platform. The core principles you will learn are transferable to any technology ecosystem. The focus is on writing clean, modular, and production-ready code that can be deployed on any platform, whether you use [GitLab](https://about.gitlab.com/) for CI/CD, [Azure DevOps](https://azure.microsoft.com/en-us/products/devops) for workflow management, or a custom internal solution.

## How does this course prepare me for any MLOps platform?

MLOps platforms are designed to work with various artifacts, from Jupyter notebooks to container images. However, a well-structured Python package is the most versatile and maintainable format for production code.

This course equips you with the skills to develop high-quality Python packages that serve as the portable core of your ML application. You will learn to use standard, powerful tools that integrate seamlessly with any MLOps platform:

-   **Testing**: With [pytest](https://docs.pytest.org/) and [coverage](https://coverage.readthedocs.io/), you can ensure your code is reliable and correct.
-   **Packaging**: By creating a distributable package, you can manage dependencies with `uv` and deploy your code consistently across environments.
-   **Distribution**: Your package can be published to repositories like [PyPI](https://pypi.org/) for easy sharing or built into a [Docker](https://www.docker.com/) image for containerized deployment.

With a solid, well-tested codebase, you can confidently integrate your work into any MLOps platform. Your packaged code can be run as a job on **Databricks**, a processing step in an **AWS SageMaker Pipeline**, or a containerized service on **Kubernetes**, allowing you to focus on the unique challenges of your project, such as data management and model orchestration.

## Additional Resources

- [The ultimate list of internal ML platforms](https://www.evidentlyai.com/ml-platforms)
