# üì¶ 3.1. Modules

Welcome to the world of production-level machine learning! As we move from notebooks to robust, maintainable code, one of the first and most crucial steps is **modularization**. This chapter will guide you through organizing your ML project into Python modules, transforming your code from a script into a professional-grade package.

## ü§î Why Modularize?

In the prototyping phase, it's common to have all your code in a single notebook or script. While this is fine for initial exploration, it quickly becomes unmanageable as the project grows. Modularization is the practice of breaking down your code into smaller, independent, and reusable files called modules.

Here‚Äôs why it‚Äôs a game-changer for MLOps:

-   **Reusable:** A `load.py` module can be used by different parts of your application, from training to batch prediction.
-   **Maintainable:** When a piece of logic needs to be updated (e.g., changing how you handle missing values), you know exactly which file to edit.
-   **Testable:** Each module can be tested independently, making it easier to catch bugs and ensure reliability.
-   **Collaborative:** Different team members can work on different modules simultaneously with fewer merge conflicts.
-   **Scalable:** As your project grows, a modular structure keeps the codebase organized and easy to navigate.

## ‚öôÔ∏è Core Operations as Modules

In a typical ML project, we can identify several core operations. We'll create a dedicated module for each of these, which will live inside our `churn` package.

Here‚Äôs the structure we‚Äôre aiming for:

```
churn/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ load.py
‚îú‚îÄ‚îÄ split.py
‚îú‚îÄ‚îÄ train.py
‚îú‚îÄ‚îÄ evaluate.py
‚îî‚îÄ‚îÄ predict.py
```

Let's break down what each module will do.

### `load.py`

This module is responsible for fetching our raw data. It could be from a local file, a database, or a cloud storage service.

*Example `churn/load.py`:*
```python
import pandas as pd

def load_data(path: str) -> pd.DataFrame:
    """Loads the dataset from a given path."""
    print(f"Loading data from {path}...")
    return pd.read_csv(path)
```

### `split.py`

Once we have the data, we need to split it into training and testing sets. This module will handle that logic.

*Example `churn/split.py`:*
```python
from typing import Tuple
import pandas as pd
from sklearn.model_selection import train_test_split

def split_data(df: pd.DataFrame, test_size: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """Splits the data into training and testing sets."""
    print("Splitting data...")
    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)
    return train_df, test_df
```

### `train.py`

This is where the magic happens! The `train.py` module will take the training data, train a model, and often save the trained model artifact.

*Example `churn/train.py`:*
```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

def train_model(X_train: pd.DataFrame, y_train: pd.Series) -> Pipeline:
    """Trains the model and returns the pipeline."""
    print("Training model...")
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', RandomForestClassifier(random_state=42))
    ])
    pipeline.fit(X_train, y_train)
    return pipeline
```

### `evaluate.py`

How good is our model? The `evaluate.py` module will use the test set to measure the model's performance.

*Example `churn/evaluate.py`:*
```python
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

def evaluate_model(model: Pipeline, X_test: pd.DataFrame, y_test: pd.Series) -> float:
    """Evaluates the model and returns the accuracy."""
    print("Evaluating model...")
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    print(f"Model Accuracy: {accuracy:.4f}")
    return accuracy
```

### `predict.py`

Finally, the `predict.py` module will use the trained model to make predictions on new, unseen data.

*Example `churn/predict.py`:*
```python
import pandas as pd
from sklearn.pipeline import Pipeline

def predict(model: Pipeline, new_data: pd.DataFrame) -> pd.Series:
    """Makes predictions on new data."""
    print("Making predictions...")
    return model.predict(new_data)
```

## üîó Connecting the Modules

These modules aren't designed to be run in isolation. They form a pipeline, where the output of one module becomes the input for the next. A master script or an orchestrator (which we'll cover later) would call them in sequence:

`load` -> `split` -> `train` -> `evaluate`

This flow ensures a clean and logical progression of data and operations.

## üêç The Role of `__init__.py`

You might be wondering about the `__init__.py` file. This file tells Python that the `churn` directory should be treated as a package. It can be empty, or it can be used to define package-level variables, import sub-modules, or run initialization code. For now, an empty `__init__.py` is all we need.

## üîë Key Takeaways

-   **Modularization is Key:** Breaking your code into modules is fundamental for creating professional, production-ready ML applications.
-   **One Module, One Job:** Each module should have a single, well-defined responsibility (e.g., loading data, training a model).
-   **Structure for Success:** A logical package structure (like our `churn` package) makes your project easy to navigate and understand.
-   **Modules Form a Pipeline:** The modules are designed to work together in a sequence to form a complete machine learning workflow.
-   **`__init__.py` Makes a Package:** This simple file is what turns a directory into a Python package that can be imported.
