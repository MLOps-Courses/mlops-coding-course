---
description: Explore the crucial concept of reproducibility in MLOps and learn how to achieve it using tools and practices. Understand the role of versioning, environment management, and experiment tracking in ensuring consistent and verifiable results.
---

# 7.0. Reproducibility

## What is reproducibility in MLOps?

[Reproducibility in MLOps](https://neptune.ai/blog/how-to-solve-reproducibility-in-ml) means being able to reliably recreate the results of an AI/ML experiment or workflow. This capability is crucial for validating findings, debugging models, and ensuring consistent behavior across different environments and over time. Reproducibility helps build trust and transparency in AI/ML projects, allowing for independent verification and accelerating future development by providing a stable foundation to build upon.

## Why is reproducibility important in MLOps?

Reproducibility is a cornerstone in any scientific endeavor, and machine learning is no exception. It ensures that results are not due to chance or specific environmental configurations. This rigor builds trust in the models, making them more reliable for deployment. Additionally, reproducibility is crucial for debugging and fixing issues. If a model's performance degrades unexpectedly, having a reproducible setup allows you to isolate the changes that caused the issue and quickly restore the model's effectiveness.

## How can you implement reproducibility in your MLOps projects?

Implementing reproducibility in MLOps projects necessitates a combination of tools and practices:

- **Code Versioning**: Utilizing tools like [Git](https://git-scm.com/) to track code changes and revert to specific versions allows you to precisely reproduce the code that generated particular results. This is essential for understanding the evolution of a model and recreating previous experiments.
- **Environment Management**: Ensuring that the environment (e.g., Python version, libraries, dependencies) in which an experiment is conducted is consistent is vital. Employing tools like [Docker](https://www.docker.com/) or [uv](https://docs.astral.sh/uv/) to encapsulate dependencies and manage environments promotes consistency and portability.
- **Dataset Versioning**: Tracking changes to the dataset used for training or evaluation is crucial. This could involve storing multiple versions of the dataset or logging metadata about the dataset's source with [MLflow Data](https://mlflow.org/docs/latest/python_api/mlflow.data.html).
- **Randomness Control**: Inherently, AI/ML tasks often involve randomness in model initialization, data shuffling, or algorithm execution. To achieve reproducibility, you must control this randomness by fixing [random seeds](https://en.wikipedia.org/wiki/Random_seed), which ensures that random number generators produce the same sequence of numbers, thereby leading to consistent results.
- **Experiment Tracking**: Employing tools like [MLflow](https://mlflow.org/) to log experiment parameters, metrics, and artifacts allows you to systematically document your experiments. This meticulous logging enables you to review past experiments, compare results, and identify the precise configurations that led to certain outcomes.

## How can you fix randomness in AI/ML frameworks?

By setting a specific seed, you ensure that the generator always produces the same sequence of "random" numbers, leading to consistent results across different executions of your code, even if those executions occur on different machines or at different times.

Here is how you can fix the randomness in your project for several popular machine learning frameworks.

### Python

```python
import random

random.seed(42)
```

### NumPy

```python
import numpy as np

np.random.seed(42)
```

### Scikit-learn

```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(random_state=42)
```

### PyTorch

```python
import torch

torch.manual_seed(42)
```

You can also fix the randomness for CUDA operations by using:

```python
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)
```

For additional reproducibility in multi-GPU environments, consider setting:

```python
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
```

### TensorFlow

```python
import tensorflow as tf

tf.random.set_seed(42)
```

## How can you build deterministic Python packages (wheels) for reproducibility?

Deterministic builds ensure that the same source code and build environment will always produce the same output. This is crucial for reproducibility in MLOps, as it guarantees that the same model or application can be rebuilt identically at any time. To achieve deterministic builds with uv, you can use the following configuration in your `justfile`:

```
# run package tasks
[group('package')]
package: package-build

# build package constraints
[group('package')]
package-constraints constraints="constraints.txt":
 uv pip compile pyproject.toml --generate-hashes --output-file={{constraints}}

# build python package
[group('package')]
package-build constraints="constraints.txt": clean-build package-constraints
 uv build --build-constraint={{constraints}} --require-hashes --wheel
```

By using the `--build-constraint` and `--require-hashes` options, uv ensures that the build process is deterministic. The `--build-constraint` option specifies a constraints file that lists the exact versions of the dependencies to be used during the build, while `--require-hashes` ensures that all dependencies are downloaded with their respective hashes, preventing any variation in the downloaded packages.

## How can you use MLflow Projects to improve the reproducibility of your project?

[MLflow Projects](https://mlflow.org/docs/latest/projects.html) is a component of MLflow that provides a standard format for packaging data science code in a reusable and reproducible way. An MLflow Project is defined by an `MLproject` file that specifies the project's dependencies, environment, and entry points. This standardized format makes it easier to share and execute projects across different environments and platforms, promoting both collaboration and consistency in project execution.

### Defining an MLflow Project

To define an [MLflow project](https://mlflow.org/docs/latest/projects.html), you can create an `MLproject` file in your project's root directory. This file uses YAML syntax to define the project structure. Below is an example of an [`MLproject`](https://github.com/fmind/mlops-python-package/blob/main/MLproject) file that specifies the project name, environment, and entry point:

```yaml
# https://mlflow.org/docs/latest/projects.html

name: bikes
python_env: python_env.yaml
entry_points:
  main:
    parameters:
      conf_file: path
    command: "PYTHONPATH=src python -m bikes {conf_file}"
```

In this example:

- `name` defines the project name as "bikes".
- `python_env` specifies the path to the [python environment file](https://github.com/fmind/mlops-python-package/blob/main/python_env.yaml).
- `entry_points` defines entry points, which specify how to run parts of the project.
  - `main` is an entry point that accepts one parameters: `conf_file` as a file path.
  - The `command` specifies how to execute the entry point, which in this case runs the `bikes` module with the provided parameters.

### Executing an MLflow Project

To run an MLflow Project:

```bash
mlflow run --experiment-name=bikes --run-name=Training -P conf_file=confs/training.yaml ."
```

This command instructs MLflow to run the current directory (`.`) as a project. The `-P` flag allows you to pass parameters to the entry points defined in your `MLproject` file. In this case, it passes `confs/training.yaml` as the main configuration file.

### Benefits of Using MLflow Projects

- **Simplified Sharing**: It's easier to share and distribute projects.
- **Consistent Execution**: Ensures consistent execution across different environments.
- **Reduced Setup Time**: Minimizes the time and effort required to set up and run projects.
- **Collaboration**: Facilitates collaboration among team members.

By leveraging MLflow Projects, you can significantly enhance the reproducibility of your MLOps projects, making it easier to share, execute, and validate your experiments, contributing to the overall robustness and trustworthiness of your ML solutions.

## Reproducibility additional resources

- **[MLflow Project example from the MLOps Python Package](https://github.com/fmind/mlops-python-package/blob/main/MLproject)**
- **[MLflow Project execution from the MLOps Python Package](https://github.com/fmind/mlops-python-package/blob/main/tasks/project.just)**
- [MLflow Projects](https://mlflow.org/docs/latest/projects.html)
