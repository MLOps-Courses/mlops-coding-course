# 2.3. Datasets

## What are datasets?

Datasets are collections of data, typically structured in a format that facilitates easy access and analysis. These can include both structured data (like tables in databases) and unstructured data (such as images or text). Metadata accompanying a dataset often describes its contents, such as column names and data types.

Example in a notebook:

```python
import pandas as pd
train = pd.read_csv('data/train.csv', index_col='Id')
print(train.shape)
train.head()
```

## Which file format should I use?

Selecting a file format for your dataset involves considering several factors:

1. **Orientation**:
    - **Row-Oriented**: Optimized for transactional operations (e.g., CSV, Avro).
    - **Column-Oriented**: Suited for analytical querying (e.g., Parquet, ORC).

2. **Structure**:
    - **Flat**: Tabular data, easy for SQL and dataframes (e.g., CSV, Excel).
    - **Hierarchical**: Nested structures, good for document databases and APIs (e.g., JSON, XML).

3. **Mode**:
    - **Textual**: Human-readable (e.g., CSV, JSON) but be aware of character encoding.
    - **Binary**: Faster and more efficient (e.g., Parquet, Avro).

4. **Density**:
    - **Dense**: Every data point is stored (e.g., CSV, Parquet).
    - **Sparse**: Only non-zero values are stored, useful for data with many empty values (e.g., SciPy sparse matrices).

## How can I optimize the dataset loading process?

To improve dataset loading and handling:
- Use **vectorization** to apply operations on entire arrays.
- Exploit **multi-core processing** for parallel computations.
- Implement **lazy evaluation** to optimize query plans.
- Consider **distributed computing** for large-scale data.

Alternatives to pandas for large datasets include:
- **[Polars](https://pola.rs/)**: Rust-based, single-machine optimized, supports lazy operations.
- **[DuckDB](https://duckdb.org/)**: C++11-based, single-machine optimized, SQL queries.
- **[Spark](https://spark.apache.org/)**: Java-based, distributed computing, lazy operations.

## Why do I need to split my dataset into 'X' and 'y'?

In supervised learning, you need to separate input variables (X) from the target variable (y). This allows models to learn to predict y based on X.

You can use pandas to separate these:

```python
X, y = train.drop('target', axis='columns'), train['target']
```

## Why should I split my dataset further into train/test sets?

Splitting data into training and test sets is [crucial for evaluating model performance](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data). It helps to:
- Avoid overfitting: ensuring the model can generalize well to new data.
- Detect underfitting: identifying if the model is too simplistic.

Use scikit-learn's `train_test_split` function to divide your dataset:

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

Remember to avoid [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)), consider [class imbalances](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis), and respect the time properties (i.e., don't predict past data with future data) of your dataset.