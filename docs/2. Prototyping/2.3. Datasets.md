---
description: Master the process of loading, exploring, and preprocessing datasets within notebooks for AI/ML projects, utilizing pandas and other tools to handle and analyze data effectively.
---

# üìä 2.3. Datasets

## ü§î What are datasets?

[Datasets](https://en.wikipedia.org/wiki/Data_set) are collections of data typically structured in a tabular format, comprising rows and columns where each row represents an observation and each column represents a feature of the observation. They are the foundational elements upon which models are trained, tested, and validated, allowing for the extraction of insights, predictions, and understandings of underlying patterns.

The quality of a dataset is paramount. High-quality data is accurate, complete, consistent, and timely. Issues like missing values, outliers, and inconsistencies can significantly degrade model performance, leading to the "garbage in, garbage out" phenomenon. Therefore, data cleaning and preprocessing are critical first steps in any machine learning project.

Here's an example of how you can [load a dataset using pandas](https://pandas.pydata.org/docs/reference/frame.html) in a notebook:

```python
import pandas as pd
# Load the dataset into a pandas DataFrame
train = pd.read_csv('data/train.csv', index_col='Id')
# Display the shape of the dataset and its first few rows
print(train.shape)
train.head()
```

![Dataset values](../img/datasets/values.png)

Datasets can originate from a wide range of sources including files (CSV, Excel, JSON, Parquet, Avro, ...), databases, and real-time data streams. They are essential for developing and testing machine learning models, conducting statistical analyses, and performing data visualization.

## üîë What are key datasets properties in pandas?

When working with datasets in pandas, several key properties enable you to quickly inspect and understand the structure and content of your data. According to the pandas documentation, the main DataFrame attributes include:

- [`.shape`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html#pandas.DataFrame.shape): Returns a tuple representing the dimensionality of the DataFrame.
- [`.dtypes`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html#pandas.DataFrame.dtypes): Provides the data types of each column.
- [`.columns`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html#pandas.DataFrame.columns): Gives an Index object containing column labels.
- [`.index`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.index.html#pandas.DataFrame.index): Returns an Index object containing row labels.

These attributes and methods are invaluable for initial data exploration and integrity checks, facilitating a deeper understanding of the dataset's characteristics.

## üìÅ Which file format should you use?

Choosing the right file format for your dataset is crucial, as it affects the efficiency of data storage, access, and processing. Consider the following criteria when selecting a file format:

| Criteria    | Options                               | Use Case                                                                              |
|-------------|---------------------------------------|---------------------------------------------------------------------------------------|
| Orientation | Row-Oriented (CSV, Avro)              | Transactional operations, frequent row-level access.                                 |
|             | Column-Oriented (Parquet, ORC)        | Analytical querying, high compression, and efficient reads.                           |
| Structure   | Flat (CSV, Excel)                     | Tabular data, easy to use with SQL and dataframes.                                    |
|             | Hierarchical (JSON, XML)              | Nested data structures, integration with document databases and APIs.                 |
| Mode        | Textual (CSV, JSON)                   | Human-readable, but be mindful of character encoding.                                 |
|             | Binary (Parquet, Avro)                | High speed and efficiency, suitable for large datasets.                               |
| Density     | Dense                                 | Every data point is stored explicitly.                                                |
|             | Sparse                                | Only non-zero values are stored, efficient for datasets with many missing values.     |


For data analytics workloads, we recommend using column-oriented, flat, binary format like the [Apache Parquet](https://parquet.apache.org/) format.

For machine learning modeling, we recommend using row-oriented, binary format based on your framework like [TFRecord for TensorFlow](https://www.tensorflow.org/tutorials/load_data/tfrecord) or [XGBoost DMatrix format](https://xgboost.readthedocs.io/en/stable/python/python_intro.html#data-interface).

## üöÄ How can you optimize the dataset loading process?

Optimizing the dataset loading process involves several strategies:

- **Vectorization**: Instead of iterating over rows, use libraries like NumPy and pandas that perform operations on entire columns or datasets. This is significantly faster because these libraries are written in C or C++ and are highly optimized.
- **Multi-core Processing**: Use libraries like Dask or Joblib to parallelize computations across multiple CPU cores. This can lead to significant speedups on multi-core machines.
- **Lazy Evaluation**: Libraries like Dask and Polars use lazy evaluation. Instead of executing operations immediately, they build a graph of computations. The computations are only executed when the final result is needed. This allows for more efficient memory usage and optimized execution plans.
- **Distributed Computing**: For datasets that don't fit into the memory of a single machine, use distributed computing frameworks like Spark or Dask. These frameworks distribute the data and computations across a cluster of machines.

For large datasets, pandas might not be sufficient. Consider alternative libraries designed for handling large-scale data efficiently:

- **[Polars](https://pola.rs/)**: A Rust-based data processing library that is optimized for performance on a single machine and supports lazy operations.
- **[DuckDB](https://duckdb.org/)**: An in-process SQL OLAP database management system that excels in analytical query performance on a single machine.
- **[Spark](https://spark.apache.org/)**: A distributed computing system that provides comprehensive support for big data processing and analytics.

The **[Ibis project](https://ibis-project.org/)** unifies these alternatives under a common interface, allowing seamless transition between different backends based on the scale of your data and computational resources (e.g., using pandas for small datasets on a laptop and Spark for big data on clusters).

## ‚úÇÔ∏è Why do you need to split your dataset into 'X' and 'y'?

In [supervised learning](https://scikit-learn.org/stable/supervised_learning.html), the convention is to split the dataset into features (`X`) and the target variable (`y`). This separation is crucial because it delineates the input variables that the model uses to learn from the output variable it aims to predict. Structuring your data this way makes it clear to both the machine learning algorithms and the developers what the inputs and outputs of themodels should be.

```
Original Dataset
+-------------------+--------+
| Feature 1         | Target |
| Feature 2         |        |
| ...               |        |
| Feature n         |        |
+-------------------+--------+
        |
        V
+-------------------+   +--------+
| Feature 1         |   | Target |
| Feature 2         |   |        |
| ...               |   |        |
| Feature n         |   |        |
+-------------------+   +--------+
      X (features)      y (target)
```

You can separate these using pandas in the following way:

```python
# Separate the dataset into features and target variable
X, y = train.drop('target', axis='columns'), train['target']
```

This practice lays the groundwork for model training and evaluation, ensuring that the algorithms have a clear understanding of the data they are working with.

## üßê Do all datasets contain potential `X` and `y` variables?

Not all datasets contain distinct `X` (features) and `y` (target) variables. These are specific to [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning). Other types of datasets and machine learning algorithms include:

- **[Time Series Forecasting](https://en.wikipedia.org/wiki/Time_series)**: Predicts future values of the same series without separate `y` targets.
- **[Unsupervised Learning](https://en.wikipedia.org/wiki/Unsupervised_learning)**: Like clustering, where data is grouped without predefined targets, or principal component analysis (PCA) which reduces dimensions without a target variable.
- **[Semi-Supervised Learning](https://en.wikipedia.org/wiki/Semi-supervised_learning)**: This approach uses a small amount of labeled data and a large amount of unlabeled data. It's a middle ground between supervised and unsupervised learning.
- **[Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning)**: Involves learning from the consequences of actions in an environment, focusing on maximizing rewards rather than predicting a target.
- **[Anomaly Detection](https://en.wikipedia.org/wiki/Anomaly_detection)**: Identifies unusual patterns or outliers without a specific target variable for training.

## üî™ Why should you split your dataset further into train/test sets?

Splitting your dataset into training and testing sets is essential for accurately evaluating the performance of your machine learning models. This approach allows you to:

- **Avoid Overfitting**: Ensuring that your model performs well not just on the data it was trained on, but also on new, unseen data.
- **Detect Underfitting**: Helping identify if the model is too simplistic to capture the underlying patterns in the data.

```
Original Dataset
+---------------------------------+
|                                 |
|                                 |
+---------------------------------+
        |
        V
+----------------+----------------+
|  Training set  |   Test set     |
| (e.g., 80%)    |  (e.g., 20%)   |
+----------------+----------------+
```

The [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from scikit-learn is commonly used for this purpose:

```python
from sklearn.model_selection import train_test_split
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

It's crucial to manage potential issues like data leakage, class imbalances, and the temporal nature of data to ensure the reliability of your model evaluations. For instance, the [Bike Sharing Demand dataset](https://www.kaggle.com/c/bike-sharing-demand) might use a [scikit-learn TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) to take into account the forecasting aspects of the project.

In addition to the training and testing sets, it's common practice to also have a **validation set**. The validation set is used to tune the model's hyperparameters and make decisions about the model's architecture. The test set is then used for the final evaluation of the model's performance on unseen data. This ensures that the test set remains a truly 'unseen' dataset.

## üé≤ Do you need to shuffle your dataset prior to splitting it into train/test sets?

Whether to shuffle your dataset before splitting it into training and testing sets depends on the nature of your problem. For time-sensitive data, such as time series, shuffling could disrupt the temporal sequence, leading to misleading training data and inaccurate models. In such cases, maintaining the chronological order is critical.

For datasets where time or sequence does not impart any context to the data, shuffling helps to ensure that the training and testing sets are representative of the overall dataset, preventing any unintentional bias that might arise from the original ordering of the data. This is especially important in scenarios where the dataset may have been sorted or is not randomly distributed (e.g., sorted by price).

## üíæ What is data versioning and why is it important?

Data versioning is the practice of tracking and managing changes to datasets over time. Just as code versioning (with tools like Git) is essential for software development, data versioning is crucial for machine learning projects.

**Why is data versioning important?**

-   **Reproducibility**: It allows you to recreate experiments by linking specific versions of your code, data, and models.
-   **Traceability**: You can trace the lineage of a model back to the exact dataset it was trained on. This is important for debugging and auditing.
-   **Collaboration**: It enables teams to work on the same data without conflicts and ensures everyone is using the correct version.
-   **Rollback**: If a new dataset introduces problems, you can easily revert to a previous, stable version.

**Tools for Data Versioning**

Several tools can help you with data versioning. One of the most popular is [**DVC (Data Version Control)**](https://dvc.org/). DVC is an open-source tool that integrates with Git to version your data and models. It works by storing metadata about your data in Git and the actual data in a remote storage like S3, Google Cloud Storage, or a network drive.

## üìù Key Takeaways

- **Data is Foundational**: Datasets are the bedrock of any AI/ML project. Ensuring data quality (accuracy, completeness) is a critical first step to avoid the "garbage in, garbage out" problem.
- **Explore Your Data**: Use pandas properties like `.shape`, `.dtypes`, `.columns`, and `.index` to quickly understand the structure and characteristics of your dataset.
- **Choose the Right File Format**: The choice of file format significantly impacts performance. Use column-oriented formats like Parquet for analytics and specialized binary formats like TFRecord for modeling.
- **Optimize Data Loading**: For large datasets, move beyond simple loading. Techniques like vectorization, parallel processing, lazy evaluation, and distributed computing (with tools like Polars, DuckDB, or Spark) are essential.
- **Structure for Supervised Learning**: Splitting your data into features (`X`) and a target (`y`) is a fundamental step for supervised machine learning tasks.
- **Splitting for Robust Evaluation**: Always split your data into training, validation, and testing sets. This helps prevent overfitting and provides a reliable measure of your model's performance on unseen data.
- **Mind the Shuffle**: Shuffling your data before splitting is a good default, but it should be avoided for time-series or sequential data where order matters.
- **Version Your Data**: Treat your data like code. Use data versioning tools like DVC to ensure reproducibility, enable collaboration, and track the lineage of your models.

## üìö Datasets additional resources

- **[Dataset example from the MLOps Python Package](https://github.com/fmind/mlops-python-package/blob/main/notebooks/prototype.ipynb)**
- [10 minutes to pandas](https://pandas.pydata.org/docs/user_guide/10min.html)
- [Scikit-learn dataset transformations](https://scikit-learn.org/stable/data_transforms.html)
- [Scikit-learn Datasets](https://scikit-learn.org/stable/datasets.html)
