# 2.5. Pipelines

## What are pipelines?

In machine learning, pipelines refer to a sequence of data processing steps. These steps encompass everything from data transformation to applying a machine learning model. Pipelines come in various forms:

- **Model Pipeline**: A sequence of steps specifically related to machine learning models (e.g., data preprocessing followed by a regression model).
  - e.g., [sklearn.pipeline.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
- **Data Pipeline**: More extensive than model pipelines, these can include data gathering, cleaning, and transformation processes.
  - e.g., [Prefect flow](https://docs.prefect.io/latest/concepts/flows/), [ZenML pipelines](https://docs.zenml.io/user-guide/starter-guide/version-pipelines), ...
- **Orchestration Pipeline**: Used for automating a series of tasks, often including data and model pipelines, in a specific order or under certain conditions.
  - e.g., [Airflow DAG](https://airflow.apache.org/docs/apache-airflow/1.10.10/concepts.html#dags), [Vertex AI](https://cloud.google.com/vertex-ai/docs/pipelines), [Prefect flow](https://docs.prefect.io/latest/concepts/flows/), [ZenML pipelines](https://docs.zenml.io/user-guide/starter-guide/version-pipelines), ...

In the context of this section, we will exclusively describe **model pipelines** as their are a core component during prototyping.

Example of a pipeline in a notebooK:

```python
pipeline = Pipeline(steps=[
    ('transformer', compose.ColumnTransformer([
        ('cat', preprocessing.OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), cat_features),
        ('num', impute.SimpleImputer(strategy='mean', add_indicator=True), num_features),
    ])),
    ('regressor', ensemble.RandomForestRegressor(random_state=42))
], memory='.cache/')
```

## Why do I need to use a pipeline?

Using a pipeline in machine learning projects has several advantages:
- **Prevents Data Leakage**: Ensures that data preprocessing steps are applied only to training data during model training.
- **Simplifies Cross-Validation**: Makes it easier to perform tasks like grid search on the correct subset of data.
- **Consistency in Training and Serving**: Guarantees the same preprocessing steps are applied during both model training and inference, avoiding discrepancies.

## Why do I need to process inputs by type?

Different types of input data often require different preprocessing steps. For example:
- **Numerical Features**: Might be scaled or normalized.
- **Categorical Features**: Usually undergo encoding like OneHotEncoding.
- **Datetime Features**: Can be decomposed into components like year, month, day.

Using tools like scikit-learn's `ColumnTransformer`, you can apply different transformations to different types of data within the same pipeline.

Note: you can select Pandas dataframe columns by types using the `df.select_dtypes()` method:

```python
num_features = X_train.select_dtypes(include=['number']).columns
cat_features = X_train.select_dtypes(include=['object']).columns
```

## What is the benefit of using a memory cache?

Using a memory cache in a pipeline (like `memory` attribute in scikit-learn's `Pipeline`) can significantly speed up operations by avoiding redundant computations, especially during tasks like grid search where certain steps might be repeated.

In this example, `ColumnTransformer` steps will be saved on disk in `.cache/` to prevent unnecessary computations:

```python
pipeline = Pipeline(steps=[
    ('transformer', compose.ColumnTransformer([
        ('cat', preprocessing.OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), cat_features),
        ('num', impute.SimpleImputer(strategy='mean', add_indicator=True), num_features),
    ])),
    ('regressor', ensemble.RandomForestRegressor(random_state=42))
], memory='.cache/')
```

## How can I change the pipeline hyper-parameters?

To adjust hyper-parameters within a scikit-learn pipeline, you use the `set_params` method or access the parameters directly using a double underscore (`__`) notation. For example:

```python
pipeline.set_params(regressor__n_estimators=100, regressor__max_depth=10)

```python
PARAM_GRID = {
    "regressor__max_depth": [12, 15, 18, 21],
    "regressor__n_estimators": [150, 200, 250, 300],
}
RANDOM_STATE = 0
pipeline = Pipeline(steps=[
    ('encoder', preprocessing.OneHotEncoder()),
    ('regressor', ensemble.RandomForestRegressor(random_state=RANDOM_STATE))
])
pipeline = pipeline.set_params(**PARAM_GRID)
```

## Why do I need to perform a grid search with my pipeline?

Grid search is essential for finding the optimal combination of hyper-parameters for your model. It systematically works through multiple combinations of parameter values, cross-validating as it goes to determine which combination gives the best performance.

```python
CV = 5
SCORING = 'neg_mean_squared_error'
PARAM_GRID = {
    "regressor__max_depth": [12, 15, 18, 21],
    "regressor__n_estimators": [150, 200, 250, 300],
}
RANDOM_STATE = 0
search = model_selection.GridSearchCV(
    estimator=draft_pipeline, param_grid=PARAM_GRID, scoring=SCORING, cv=CV, verbose=1
)
search.fit(X_train, y_train)
```

## Why do I need to perform a cross-validation with my pipeline?

Cross-validation is crucial for assessing the predictive performance of your models. It helps ensure that your model not only performs well on your training data but also generalizes well to unseen data.

This search parameter can be controlled in [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) from scikit-learn using the CV parameter:

```text
cv : int, default=None. Determines the cross-validation splitting strategy.

Possible inputs for cv are:
- None: to use the default 5-fold cross validation,
- integer: to specify the number of folds in a (Stratified)KFold,
- CV splitter: an scikit-learn splitter able to split a dataset.
- An iterable: yielding (train, test) splits as arrays of indices.
```

For simple problems with no class imbalance or time dependency, the default approach 5 fold should be enough.

For more complex problem you might need to adapt your strategy by providing a custom CV splitter suited to your problem.

Note: having a strong cross-validation strategy is [key to win data sciences competitions, such as the ones hosted on Kaggle](https://www.youtube.com/watch?v=0ZJQ2Vsgwf0).

## Do I need to retrain my pipeline? Should I use the full dataset?

Once you've determined the best hyper-parameters and validated your model with cross-validation, it's a common practice to retrain your model on the full dataset. This leverages the maximum amount of data available, potentially improving the model's performance.

You can either retrain the pipeline manually:

```python
final_pipeline = pipeline.set_params(**search.best_params_)
final_pipeline.fit(X, y)
```

Or use the best estimator from scikit-learn [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) if you set `refit=True` (default):

```python
final_pipeline = search.best_estimator_
```

Note: the best estimator is retrained on the whole dataset passed to grid-search (i.e., X_train, y_test).
