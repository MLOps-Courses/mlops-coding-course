# 2.6. Evaluation

## What is an evaluation?

An evaluation is a critical process for understanding and validating a machine-learning model and its predictions. It serves as a quality check, providing a level of assurance before deploying the model in a production environment. Evaluations can be presented in various formats, such as tables showing prediction errors or graphical representations like validation curves. This process is essential for ensuring the reliability and accuracy of your model.

## Why should I evaluate my pipeline?

Machine-learning models, owing to their complexity, can exhibit unpredictable behaviors. Evaluating your training pipeline helps in identifying potential issues, such as data leakage, which can compromise the model's ability to generalize to new data. Trust and credibility are paramount in data science. Evaluating your model rigorously ensures that its performance is not merely superficial but robust and reliable.

For more insights on data leakage, explore this link: [Data Leakage in Machine Learning](https://en.wikipedia.org/wiki/Leakage_(machine_learning)).

## What do I need to evaluate in my pipeline?

Evaluating your training involves several key aspects. Let's delve into some of the most crucial ones:

### Ranks

Analyzing the ranks obtained during hyper-parameter tuning is vital. Investigate whether:
- Certain hyper-parameter combinations are ineffective and should be eliminated.
- The best hyper-parameters are outliers or commonly occurring in your rank distribution.

This evaluation helps determine whether to expand or narrow your search space.

**Example**:
```python
px.line(results, x='rank_test_score', y='mean_test_score', title='Rank by test score')
```

### Params

Assessing the hyper-parameters that yield better results is crucial. Look for:
- Trends indicating optimal settings (e.g., higher `max_depth` leading to better performance).
- Hyper-parameters with negligible impact, which could be excluded.

This analysis helps identify the most effective hyper-parameters for your specific problem.

**Example**:
```python
dimensions = [col for col in results.columns if col.startswith('param_')]
px.parallel_categories(results, dimensions=dimensions, color='mean_test_score', title="Params by test score")
```

### Predictions

Evaluate the prediction values obtained by your machine-learning model. You might investigate:
- if the value distribution is similar to the one found in your training set
- if there are some potential imbalance (e.g., more high values compared to low values)

You should expect to have similar distribution and balance between your training examples and predictions.

**Example**:
```python
y_pred = pd.Series(final_pipeline.predict(X_test), index=X_test.index)
score = metrics.mean_squared_error(y_test, y_pred)
score
```

### Prediction Errors

Evaluating prediction values is essential. Consider:
- Whether the distribution of values mirrors that of the training set.
- Any potential imbalances, such as a skew towards high or low values.

Expect similar distributions and balance between your training examples and predictions.

**Example**:
```python
errors = pd.concat([y_test, y_pred], axis=1, keys=["y_test", "y_pred"])
errors["error"] = errors["y_pred"] - errors["y_test"]

px.histogram(errors, x="error", title="Distribution of errors")
```

### Feature Importance

Evaluating feature importance helps understand the influence of each variable in your model. Determine:
- Which features significantly impact your model's performance.
- Features that could be eliminated to reduce complexity without substantially affecting accuracy.

Remember, some models (like [linear](https://scikit-learn.org/stable/modules/linear_model.html) and [tree](https://scikit-learn.org/stable/modules/tree.html) models) are more amenable to this kind of analysis.

**Example**:
```python
importances = pd.Series(
    final_pipeline.named_steps['regressor'].feature_importances_,
    index=final_pipeline[:-1].get_feature_names_out(),
).sort_values(ascending=False)
px.bar(importances, title="Feature importances")
```

## How can I ensure my pipeline was trained on enough data?

Use a [learning curve](https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)) to understand the relationship between the volume of training data and model performance. Keep adding diverse data until the model's performance plateau, indicating you have reached an optimal amount of data.

**Example using scikit-learn's learning curve**:
```python
train_size_abs, train_scores, test_scores = model_selection.learning_curve(final_pipeline, X, y, cv=CV, scoring=SCORING, random_state=RANDOM_STATE)
learning = pd.DataFrame({
    "train_size": train_size_abs,
    "mean_test_score": test_scores.mean(axis=1),
    "mean_train_score": train_scores.mean(axis=1),
})
px.line(learning, x='train_size', y=['mean_test_score', 'mean_train_score'], title="Learning Curve")
```

## How can I ensure my pipeline captures the right level of complexity?

To ensure your pipeline captures the right level of complexity, plot a [validation curve](https://scikit-learn.org/stable/modules/learning_curve.html). This curve shows how varying a parameter (e.g., increasing depth) affects model performance. Increase complexity as long as the model improves without overfitting.

**Example with scikit-learn's validation curve**:
```python
PARAM_NAME = 'regressor__n_estimators'
PARAM_RANGE = PARAM_GRID[PARAM_NAME]
train_scores, test_scores = model_selection.validation_curve(
    final_pipeline, X, y, cv=CV, scoring=SCORING, param_name=PARAM_NAME, param_range=PARAM_RANGE,
)
validation = pd.DataFrame(
    {
        "param_value": PARAM_RANGE,
        "mean_test_score": test_scores.mean(axis=1),
        "mean_train_score": train_scores.mean(axis=1),
    }
)
px.line(validation, x="param_value", y=["mean_test_score", "mean_train_score"], title=f"Validation Curve for: {PARAM_NAME}")
```