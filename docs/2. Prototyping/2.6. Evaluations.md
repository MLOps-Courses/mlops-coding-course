---
description: Discover methods for effectively evaluating model performance using various metrics and visualizations to ensure your models are robust and generalizable.
---

# üìà 2.6. Evaluations

Model evaluation is the cornerstone of building reliable machine learning systems. It's the process of rigorously assessing your model's performance to ensure it's not just making predictions, but making *good* predictions. This chapter will guide you through the essential techniques for evaluating your models, from interpreting performance metrics to visualizing results, so you can be confident in the solutions you deploy.

## ü§î Why is Evaluation Crucial?

In machine learning, a model can sometimes feel like a "black box." You feed it data, and it gives you answers. But how do you know if those answers are trustworthy? Evaluation is your window into the model's behavior. It helps you:

- **Build Trust:** By systematically checking performance, you can verify that your model works as expected and build confidence in its predictions.
- **Avoid Pitfalls:** It allows you to catch common issues like data leakage, where the model accidentally "cheats" by learning from data it shouldn't have access to, leading to poor generalization on new, unseen data.
- **Make Informed Decisions:** Evaluation metrics provide the quantitative feedback needed to compare different models or hyperparameter settings, guiding you toward the optimal solution.

For a deeper dive into data leakage, check out this article: [Data Leakage in Machine Learning](https://en.wikipedia.org/wiki/Leakage_(machine_learning)).

## ü§ñ Generating Predictions

The first step in evaluation is to generate predictions on a dataset the model has never seen before‚Äîthe **hold-out set** (or test set). This simulates how the model will perform in a real-world scenario.

```python
# Generate predictions on the test set
predictions = pd.Series(final_pipeline.predict(inputs_test), index=inputs_test.index)
print(f"Predictions shape: {predictions.shape}")
predictions.head()
```

It's also insightful to analyze the results from your hyperparameter tuning process. This can reveal which parameter combinations worked best.

```python
# Analyze hyperparameter tuning results
results = pd.DataFrame(search.cv_results_)
results = results.sort_values(by="rank_test_score")
results.head()
```

## üîç What to Evaluate in Your Pipeline

A thorough evaluation goes beyond a single accuracy score. Here are key areas to investigate:

### üèÖ Ranks

When you run hyperparameter tuning, you're searching for the best combination of settings. Analyzing the ranks of different runs can tell you:

- **Ineffective Combinations:** Quickly identify and discard hyperparameters that consistently perform poorly.
- **Optimal Trends:** Determine if the best-performing hyperparameters are outliers or part of a broader, promising trend. This helps you decide whether to narrow your search to a specific region or expand it.

```python
# Visualize rank vs. mean test score
px.line(results, x="rank_test_score", y="mean_test_score", title="Rank by Test Score")
```

![Evaluation ranks](../img/evaluations/rank.png)

*__Interpretation:__ This plot helps you see how performance changes with rank. A steep drop-off might indicate that only a few hyperparameter sets are effective.*

### ‚öôÔ∏è Params

Digging into the hyperparameters themselves can reveal which ones have the most impact on performance. Look for:

- **Performance-Driving Params:** Identify trends that suggest optimal values or ranges for certain hyperparameters.
- **Insignificant Params:** Spot hyperparameters that have little to no effect on the outcome. You might consider removing these from your search to simplify the model.

```python
# Visualize hyperparameter impact on test score
dimensions = [col for col in results.columns if col.startswith("param_")]
px.parallel_categories(results, dimensions=dimensions, color="mean_test_score", title="Params by Test Score")
```

![Evaluation params](../img/evaluations/params.png)

*__Interpretation:__ This parallel categories plot shows how different hyperparameter values correlate with the mean test score. Follow the lines to see which combinations lead to better (or worse) outcomes.*

### üîÆ Predictions & Errors

Analyzing the model's predictions and errors is crucial.

**For regression tasks**, you can calculate metrics like Mean Squared Error (MSE):

```python
# Calculate a performance metric (e.g., MSE)
score = metrics.mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {score:.4f}")
```

Visualizing the distribution of errors can also be insightful. Are the errors centered around zero? Are there outliers?

```python
# Visualize distribution of errors
errors = pd.DataFrame({"error": y_test - y_pred})
px.histogram(errors, x="error", title="Distribution of Prediction Errors")
```

![Evaluation errors](../img/evaluations/errors.png)

*__Interpretation:__ An ideal error distribution is centered at zero with a symmetric, bell-like shape, indicating that the model's errors are unbiased.*

**For classification tasks**, you can use tools like a **confusion matrix** to see where the model is getting confused. Other key metrics include:
- **Precision:** What proportion of positive identifications was actually correct?
- **Recall:** What proportion of actual positives was identified correctly?
- **F1-Score:** A weighted average of precision and recall.

### ‚ú® Feature Importances

Understanding which features have the biggest impact on your model's predictions can help you:
- **Simplify Your Model:** You may be able to remove features with low importance, making your model faster and more interpretable.
- **Gain Insights:** It can provide valuable domain insights by highlighting the key drivers of the outcome.

This is often easiest with linear models and tree-based ensembles.

```python
# Determine and visualize feature importances
importances = pd.Series(
    final_pipeline.named_steps["regressor"].feature_importances_,
    index=final_pipeline[:-1].get_feature_names_out(),
).sort_values(ascending=False)

px.bar(importances, title="Feature Importances")
```

![Evaluation feature importances](../img/evaluations/feature_importances.png)

*__Interpretation:__ This bar chart ranks features by their importance. The higher the bar, the more influential the feature is in the model's predictions.*

## üìä Is Your Pipeline Trained on Enough Data?

A **learning curve** helps you see how the model's performance changes as the size of the training dataset increases. This can tell you if you'd benefit from collecting more data.

```python
# Analyze the learning curve
train_size, train_scores, test_scores = model_selection.learning_curve(
    final_pipeline, inputs, targets, cv=splitter, scoring=SCORING, random_state=RANDOM,
)
learning = pd.DataFrame({
    "train_size": train_size,
    "mean_test_score": test_scores.mean(axis=1),
    "mean_train_score": train_scores.mean(axis=1),
})
px.line(learning, x="train_size", y=["mean_test_score", "mean_train_score"], title="Learning Curve")
```

![Evaluation learning curve](../img/evaluations/learning_curve.png)

*__Interpretation:__ If the training and validation scores have converged and are low, you may have high bias (underfitting). If there's a large gap between the two scores, you may have high variance (overfitting). If both scores are still improving with more data, collecting more data could be beneficial.*

## ‚öñÔ∏è Does Your Pipeline Have the Right Complexity?

A **validation curve** is used to see how a single hyperparameter (like the `max_depth` of a tree) affects the model's training and validation performance. This helps you find the sweet spot between an overly simple (underfit) and an overly complex (overfit) model.

```python
# Explore validation curves for different hyperparameters
for param_name, param_range in PARAM_GRID.items():
    train_scores, test_scores = model_selection.validation_curve(
        final_pipeline, inputs, targets, cv=splitter, scoring=SCORING,
        param_name=param_name, param_range=param_range,
    )
    validation = pd.DataFrame({
        "param_value": param_range,
        "mean_test_score": test_scores.mean(axis=1),
        "mean_train_score": train_scores.mean(axis=1),
    })
    fig = px.line(
        validation, x="param_value", y=["mean_test_score", "mean_train_score"],
        title=f"Validation Curve: {param_name}"
    )
    fig.show()
```

![Evaluation validation curve](../img/evaluations/validation_curve.png)

*__Interpretation:__ Look for the point where the validation score is maximized. If the training score is high but the validation score is low, your model is likely overfitting.*

## üîë Key Takeaways

- **Evaluate on Unseen Data:** Always use a separate test set to get an unbiased estimate of your model's performance.
- **Go Beyond Single Metrics:** A single number won't tell you the whole story. Use a combination of metrics and visualizations to get a complete picture.
- **Visualize, Visualize, Visualize:** Plots like learning curves, validation curves, and feature importance charts provide deep, intuitive insights into your model's behavior.
- **Hyperparameter Tuning is Key:** Systematically search for the best hyperparameters and analyze the results to understand what works.
- **Connect Evaluation to Action:** Use the insights from your evaluation to make concrete improvements to your model, whether it's collecting more data, simplifying features, or adjusting complexity.

## üìö Additional Resources

- **[Example from the MLOps Python Package](https://github.com/fmind/mlops-python-package/blob/main/notebooks/prototype.ipynb)**
- [Data Leakage in Machine Learning](https://en.wikipedia.org/wiki/Leakage_(machine_learning))
- [Scikit-learn Model Selection and Evaluation](https://scikit-learn.org/stable/model_selection.html)
