---
description: Learn how to build, refine, and compare machine learning models directly within notebooks, covering everything from initial prototypes to model selection and hyperparameter tuning.
---

# 2.5. Modeling

## ü§î What are pipelines?

[Pipelines in machine learning](https://scikit-learn.org/stable/modules/compose.html#pipeline) provide a streamlined way to organize sequences of data preprocessing and modeling steps. They encapsulate a series of data transformations followed by the application of a model, facilitating both simplicity and efficiency in the development process. Pipelines can be broadly categorized as follows:

- **Model Pipeline**: Focuses on preparing data for and applying machine learning models. A typical example is a `scikit-learn` pipeline that chains preprocessing steps (like scaling and encoding) with a final estimator (like a classifier or regressor).
- **Data Pipeline**: Covers a broader range of data-related tasks, including extraction, transformation, and loading (ETL). These pipelines gather data from various sources, clean and process it, and load it into a destination like a data warehouse. Tools like `Prefect` and `ZenML` are designed for building robust data pipelines.
- **Orchestration Pipeline**: Manages and automates a series of tasks, which can include both data and model pipelines. Orchestration tools ensure that tasks run in the correct order, handle dependencies, and manage resources. `Apache Airflow` and `Vertex AI` are popular examples for creating and managing complex workflows.

For the purposes of this discussion, we'll focus on **model pipelines**, crucial for efficiently prototyping machine learning solutions. The code example are based on [scikit-learn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), as this toolkit is simple to understand and its concept can be generalized to other types of pipeline like [Dagster](https://dagster.io/), [Prefect](https://www.prefect.io/), or [Metaflow](https://metaflow.org/).

Example of defining a pipeline in a notebook:

```python
from sklearn import pipeline, compose, preprocessing, ensemble

categoricals = [...] # List of categorical feature names
numericals = [...] # List of numerical feature names
RANDOM = 42 # Fixed random state for reproducibility
CACHE = './.cache' # Path for caching transformers

# Constructing a pipeline
draft = pipeline.Pipeline(
    steps=[
        ("transformer", compose.ColumnTransformer([
            ("categoricals", preprocessing.OneHotEncoder(
                sparse_output=False, handle_unknown="ignore"
            ), categoricals),
            ("numericals", "passthrough", numericals),
        ], remainder="drop")),
        ("regressor", ensemble.RandomForestRegressor(random_state=RANDOM)),
    ],
)
```

![Model pipeline](../img/models/pipeline.png)

## üöÄ Why do you need to use a pipeline?

Implementing pipelines in your machine learning projects offers several key advantages:

- **Prevents Data Leakage**: Pipelines ensure that preprocessing steps are applied separately to the training and validation sets within each fold of cross-validation. This prevents information from the validation set from "leaking" into the training process, which can lead to overly optimistic performance estimates.
- **Simplifies Cross-Validation**: With a pipeline, you can treat a sequence of preprocessing steps and a model as a single object. This makes it much easier to perform cross-validation, as you only need to call `fit` and `predict` on the pipeline itself, rather than on each individual component.
- **Ensures Consistency**: A pipeline guarantees that the same preprocessing steps are applied to both your training data and any new data you want to make predictions on. This consistency is crucial for ensuring that your model behaves as expected in production.
- **Improves Reproducibility**: By encapsulating the entire workflow, pipelines make it easier for others (and your future self) to reproduce your results.

In short, pipelines are a fundamental tool for building robust and reliable machine learning models.

## üé® Why do you need to process inputs by type?

Different data types require distinct preprocessing steps to be effectively used by machine learning models. Here‚Äôs a breakdown of why and how:

- **Numerical Features**: These features often need to be scaled to a common range (e.g., 0 to 1) or standardized to have a mean of 0 and a standard deviation of 1. This is important for algorithms that are sensitive to the scale of the input features, such as Support Vector Machines (SVMs) and k-Nearest Neighbors (k-NN).
- **Categorical Features**: Machine learning models can only work with numerical data, so categorical features (like `red`, `green`, `blue`) must be converted into a numerical format. Common techniques include one-hot encoding, which creates a new binary feature for each category, and ordinal encoding, which assigns a unique integer to each category.
- **Datetime Features**: These features are rich in information but need to be broken down into more granular components that models can understand. For example, you can extract the year, month, day of the week, or even the hour of the day to capture temporal patterns.
- **Text Features**: Text data needs to be converted into a numerical representation, often using techniques like TF-IDF or word embeddings (e.g., Word2Vec, GloVe).

The `ColumnTransformer` in `scikit-learn` is a powerful tool that allows you to apply different preprocessing steps to different columns of your data. This ensures that each feature type is handled appropriately, which is crucial for building accurate and reliable models.

Example of [selecting features by type from a Pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html):

```python
import pandas as pd

# Assume X_train is your training data stored in a Pandas DataFrame
num_features = X_train.select_dtypes(include=['number']).columns.tolist()
cat_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
```

## ‚ö°Ô∏è What is the benefit of using a memory cache?

[Employing a memory cache with pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), such as the `memory` attribute in `scikit-learn`'s `Pipeline`, can dramatically speed up your workflow. Caching stores the results of a transformation step, so that it doesn't have to be recomputed every time the pipeline is run.

This is especially useful in scenarios like:

- **Grid Search**: When performing a grid search, you are fitting the same pipeline multiple times with different hyperparameters. If your preprocessing steps are computationally expensive, caching them can save a significant amount of time.
- **Iterative Development**: When you are experimenting with different models or hyperparameters, you often need to re-run your pipeline multiple times. Caching the initial transformation steps means you only pay the computational cost once.

By caching the results of your transformers, you can make your development process faster and more efficient, allowing you to iterate more quickly and focus on the modeling aspect of your project.

Example of utilizing a memory cache with a pipeline:

```python
from sklearn import pipeline, compose, preprocessing, ensemble

# Assuming 'categoricals' and 'numericals' are defined as before
CACHE = './.cache' # Directory for caching transformers

# Constructing the pipeline with caching enabled
draft = pipeline.Pipeline(
    steps=[
        ("transformer", compose.ColumnTransformer([
            ("categoricals", preprocessing.OneHotEncoder(
                sparse_output=False, handle_unknown="ignore"
            ), categoricals),
            ("numericals", "passthrough", numericals),
        ], remainder="drop")),
        ("regressor", ensemble.RandomForestRegressor(random_state=RANDOM)),
    ],
    memory=CACHE,
)
```

Even if you don't plan on using [scikit-learn pipeline abstraction](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), you can implement the same concept in your code base to obtain the same benefits.

## üõ†Ô∏è How can you change the pipeline hyper-parameters?

Adjusting hyperparameters within a [scikit-learn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) is a common task, and `scikit-learn` provides a convenient way to do this using the `set_params` method. This method allows you to change the hyperparameters of any step in the pipeline, whether it's a transformer or a model.

The key to `set_params` is the double underscore (`__`) notation. You use it to specify the name of the step, followed by the name of the hyperparameter you want to change. For example, `regressor__n_estimators` refers to the `n_estimators` hyperparameter of the `regressor` step.

This is particularly useful when you want to programmatically change hyperparameters, for example, in a loop or as part of a grid search. It allows you to fine-tune your model directly within the pipeline structure, without having to manually recreate the pipeline every time you want to try a new set of hyperparameters.

Example of setting pipeline hyper-parameters:

```python
from sklearn.pipeline import Pipeline
from sklearn import preprocessing, ensemble

# Assume 'RANDOM_STATE' and 'PARAM_GRID' are defined
pipeline = Pipeline([
    ('encoder', preprocessing.OneHotEncoder()),
    ('regressor', ensemble.RandomForestRegressor(random_state=RANDOM_STATE))
])

# Adjusting hyper-parameters using 'set_params'
pipeline.set_params(regressor__n_estimators=100, regressor__max_depth=10)
```

## üîç Why do you need to perform a grid search with your pipeline?

Conducting a [grid search over a pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) is a powerful technique for finding the best combination of hyperparameters for your model. It works by exhaustively searching through a specified set of hyperparameter values and evaluating each combination using cross-validation.

When you perform a grid search on a pipeline, you can tune the hyperparameters of both the preprocessing steps and the model itself. This is important because the optimal hyperparameters for your model may depend on the preprocessing steps you apply.

For example, you could search over different encoding strategies for your categorical features, different scaling methods for your numerical features, and different hyperparameters for your model, all at the same time. This allows you to find the best possible combination of preprocessing and modeling choices for your specific problem.

Example of performing grid search with a pipeline:

```python
from sklearn.model_selection import GridSearchCV
from sklearn import model_selection

CV = 5
SCORING = 'neg_mean_squared_error'
PARAM_GRID = {
    "regressor__max_depth": [15, 20, 25],
    "regressor__n_estimators": [150, 200, 250],
}

splitter = model_selection.TimeSeriesSplit(n_splits=CV)

search = GridSearchCV(
    estimator=draft, cv=splitter, param_grid=PARAM_GRID, scoring=SCORING, verbose=1
)
search.fit(inputs_train, targets_train)
```

## üìä Why do you need to perform cross-validation with your pipeline?

[Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is a powerful technique for assessing how well your model will generalize to unseen data. It works by splitting your training data into a number of "folds," and then training and evaluating your model multiple times, using a different fold as the validation set each time.

When you use cross-validation with a pipeline, you ensure that the entire workflow, including preprocessing, is evaluated at each step. This gives you a more reliable estimate of your model's performance than a simple train-test split.

`scikit-learn`'s `GridSearchCV` automatically performs cross-validation for you, but you can also use other cross-validation strategies, such as `TimeSeriesSplit` for time-series data, or `StratifiedKFold` for classification problems with imbalanced classes. The `cv` parameter in `GridSearchCV` allows you to specify the cross-validation strategy that is most appropriate for your problem.

When utilizing [`GridSearchCV` from scikit-learn for hyperparameter tuning](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), the `cv` parameter plays a crucial role in defining the cross-validation splitting strategy. This flexibility allows you to tailor the cross-validation process to the specific needs of your dataset and problem domain, ensuring that the model evaluation is both thorough and relevant.

Here‚Äôs a breakdown of how you can control the cross-validation behavior through the `cv` parameter:

- **`None`**: By default, or when `cv` is set to `None`, GridSearchCV employs a 5-fold cross-validation strategy. This means the dataset is divided into 5 parts, with the model being trained on 4 parts and validated on the 1 remaining part in each iteration.

- **Integer**: Specifying an integer for `cv` changes the number of folds in a K-Fold (or StratifiedKFold for classification tasks) cross-validation. For example, `cv=10` would perform a 10-fold cross-validation, offering a more thorough validation at the cost of increased computational time.

- **CV Splitter**: scikit-learn provides several splitter classes (e.g., `KFold`, `StratifiedKFold`, `TimeSeriesSplit`) that can be used to define more complex cross-validation strategies. Passing an instance of one of these splitters to `cv` allows for customized dataset splitting that can account for factors like class imbalance or temporal dependencies.

- **Iterable**: An iterable yielding train/test splits as arrays of indices directly specifies the data partitions for each fold. This option offers maximum flexibility, allowing for completely custom splits based on external logic or considerations (e.g., predefined groups or stratifications not captured by the standard splitters).

## üîÑ Do you need to retrain your pipeline? Should you use the full dataset?

After identifying the best model and hyperparameters through grid search and cross-validation, it is a common practice to retrain the model on the entire dataset. This is because the more data a model is trained on, the better it is likely to perform.

By default, `scikit-learn`'s `GridSearchCV` automatically refits the best model on the entire dataset after the grid search is complete. This means that the `best_estimator_` attribute of the `GridSearchCV` object is a model that has been trained on all of the available data.

However, it's important to remember that once you've retrained your model on the full dataset, you no longer have a separate validation set to evaluate its performance. This is why it's so important to have a robust cross-validation strategy in the first place. The cross-validation results give you a good estimate of how well your final model will perform on unseen data.

Example of retraining your pipeline on the full dataset:

```python
# Assuming 'search' is your GridSearchCV object and 'X', 'y' are your full dataset
final_model = search.best_estimator_
final_model.fit(X, y)
```

Alternatively, if you've used [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) with `refit=True` (which is the default setting), the best estimator is automatically refitted on the whole dataset provided to `fit`, making it ready for use immediately after grid search:

```python
# 'search' has been conducted with refit=True
final_model = search.best_estimator_
```

In this way, the final model embodies the culmination of your exploratory work, tuned hyper-parameters, and the comprehensive learning from the entire dataset, positioning it well for effective deployment in real-world applications.

It's important to note, however, that while retraining on the full dataset can improve performance, it also eliminates the possibility of evaluating the model on unseen data unless additional, separate validation data is available. Therefore, the decision to retrain should be made with consideration of how model performance will be assessed and validated post-retraining.

## üìö Modeling additional resources

- **[Example from the MLOps Python Package](https://github.com/fmind/mlops-python-package/blob/main/notebooks/prototype.ipynb)**
- [Supervised learning](https://scikit-learn.org/stable/supervised_learning.html)
- [Unsupervised learning](https://scikit-learn.org/stable/unsupervised_learning.html)
- [HuggingFace Models](https://huggingface.co/models)
- [Kaggle Models](https://www.kaggle.com/models)

## üîë Key Takeaways

- **Pipelines are Essential**: They streamline the workflow, prevent data leakage, and simplify complex sequences of transformations and modeling.
- **Process by Type**: Always preprocess features based on their data type (numerical, categorical, etc.) to ensure models can interpret them correctly.
- **Cache for Speed**: Use memory caching in pipelines to avoid redundant computations, especially during hyperparameter tuning, which significantly speeds up the process.
- **Tune Hyperparameters**: Use `set_params` and grid search to systematically find the best hyperparameters for your pipeline.
- **Cross-Validate Thoroughly**: Employ cross-validation to get a reliable estimate of your model's performance on unseen data.
- **Retrain on Full Data**: After finding the best model, retrain it on the entire dataset to maximize its predictive power.
- **Scikit-learn is Powerful**: `scikit-learn` provides a comprehensive toolkit for building, evaluating, and tuning machine learning pipelines.
