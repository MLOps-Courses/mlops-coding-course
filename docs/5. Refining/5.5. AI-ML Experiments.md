---
description: Master AI/ML experiment tracking with MLflow. Learn how to effectively manage your experiments, track model performance, and compare results to identify the best configurations and optimize your machine learning models.
---

# 5.5. AI/ML Experiments

<iframe class="youtube" width="560" height="315" src="https://www.youtube.com/embed/7AkZvvlgNJQ?si=0_VLPj6OEUcVMUQq" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## What is an AI/ML experiment?

[An AI/ML experiment](https://neptune.ai/blog/ml-experiment-tracking) is a systematic and iterative process for building robust machine learning models. It involves testing different algorithms, tuning hyperparameters, and using various datasets to discover the optimal configuration for a specific predictive task. Each experiment is a structured trial designed to measure the impact of changes on model performance, such as accuracy, efficiency, and reliability.

## Why is experiment tracking essential in AI/ML?

In MLOps, the complexity and often non-deterministic nature of model development require a disciplined approach. Experiment tracking provides the necessary structure, much like a lab notebook for a scientist. Key benefits include:

- **Ensuring Reproducibility**: Tracking guarantees that every aspect of an experiment—code, data, environment, and parameters—is recorded. This allows you and your team to reliably replicate and verify results.
- **Optimizing Hyperparameters**: It provides a systematic way to test and compare different hyperparameter configurations, helping you pinpoint the settings that maximize model performance.
- **Organizing Your Work**: By logging experiments and using tags, you can categorize runs by model type, dataset, or objective. This organization is crucial for managing complex projects and quickly retrieving past results.
- **Monitoring Performance**: Tracking metrics during a run offers real-time insight into how adjustments affect model behavior, enabling faster, data-driven decisions.
- **Seamless Framework Integration**: Modern tracking tools integrate with popular AI/ML frameworks, creating a unified and streamlined workflow across your entire toolchain.

## Which experiment tracking solution should you use?

Numerous solutions are available for tracking AI/ML experiments. Major cloud platforms like [Google Cloud (Vertex AI)](https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments), [Azure (Azure ML)](https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py), and [AWS (SageMaker)](https://aws.amazon.com/fr/sagemaker/experiments/) offer powerful, integrated MLOps capabilities. Specialized commercial tools like [Weights & Biases](https://wandb.ai/) and [Neptune AI](https://neptune.ai/) also provide excellent features.

For those starting out or preferring an open-source, framework-agnostic solution, [MLflow](https://mlflow.org/) is an outstanding choice. It is versatile, robust, and integrates with a wide array of ML libraries.

To install MLflow, run:

```bash
uv add mlflow
```

To verify the installation and start the MLflow UI server locally:

```bash
uv run mlflow doctor
uv run mlflow server
```

For a more permanent setup using Docker, you can use a `docker-compose.yml` file to launch the MLflow server:

```yaml
services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.14.1
    ports:
      - 5000:5000
    environment:
      - MLFLOW_HOST=0.0.0.0
    command: mlflow server
```

Run `docker compose up` to start the service. For information on production-grade deployments, refer to the [MLflow documentation](https://mlflow.org/docs/latest/tracking.html#set-up-the-mlflow-tracking-environment).

## How do you configure MLflow in a project?

To integrate MLflow, you first need to configure it to store experiment data. You can start by setting the tracking and registry URIs to a local directory, such as `./mlruns`. Then, define an experiment name to group related runs.

Enabling [MLflow's autologging](https://mlflow.org/docs/latest/tracking/autolog.html) is highly recommended. It automatically captures metrics, parameters, and models from popular ML libraries without requiring explicit logging statements.

```python
import mlflow

# Configure MLflow to save data to a local directory
mlflow.set_tracking_uri("file://./mlruns")
mlflow.set_registry_uri("file://./mlruns")

# Set a name for the experiment
mlflow.set_experiment(experiment_name="Bike Sharing Demand Prediction")

# Enable autologging for automatic tracking
mlflow.autolog()
```

To start a new run, wrap your training code within an [MLflow run context](https://mlflow.org/docs/latest/tracking.html#tracking-runs). This allows you to add descriptive metadata and enable system metric logging.

```python
with mlflow.start_run(
    run_name="Forecast Model with Feature Engineering",
    description="Training a model with an enhanced feature set.",
    log_system_metrics=True,
) as run:
    # Your model training and evaluation code goes here
    print(f"MLflow Run ID: {run.info.run_id}")
```

## What information can you track in an experiment?

While [MLflow's autologging](https://mlflow.org/docs/latest/tracking/autolog.html) captures a wealth of information automatically, you can enhance it with [manual logging](https://mlflow.org/docs/latest/tracking/tracking-api.html#manual-logging) for greater detail:

- **Parameters**: Log key-value parameters with [`mlflow.log_param()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_param) or a dictionary of parameters with [`mlflow.log_params()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_params).
- **Metrics**: Record single metrics over time (e.g., per epoch) with [`mlflow.log_metric()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metric) or a dictionary of metrics with [`mlflow.log_metrics()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metrics).
- **Inputs**: Log dataset details and context with [`mlflow.log_input()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_input), including tags for better categorization.
- **Tags**: Assign custom tags to a run for improved filtering and organization using [`mlflow.set_tag()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_tag) or [`mlflow.set_tags()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_tags).
- **Artifacts**: Save output files, such as plots, model files, or data samples, with [`mlflow.log_artifact()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact) for single files or [`mlflow.log_artifacts()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifacts) for directories.

## How can you compare experiments to find the best model?

Comparing experiments is essential for model selection. MLflow provides two powerful ways to do this: its [web UI](https://mlflow.org/docs/latest/tracking.html#tracking-ui) and its [programmatic API](https://mlflow.org/docs/latest/tracking.html#querying-runs-programmatically).

### Comparing via the MLflow Web UI

The MLflow UI offers an intuitive, visual way to compare runs.

1.  **Launch the MLflow Server**: If it's not running, start it with `uv run mlflow server`.
2.  **Select Runs**: Navigate to the experiment page, where all runs are listed. Use the checkboxes to select the runs you want to compare.
3.  **Click Compare**: A "Compare" button will appear. Clicking it opens a detailed view that places the selected runs side-by-side.
4.  **Analyze Results**: This view provides a comprehensive summary of parameters, metrics, and artifacts for each run. You can use it to identify which configurations yielded the best performance.

### Comparing Programmatically

Programmatic comparison is ideal for automated analysis and custom reporting.

1.  **Query Runs**: Use [`mlflow.search_runs()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.search_runs) to fetch run data into a pandas DataFrame. You can filter by experiment, metrics, parameters, or tags.

    ```python
    import mlflow

    # Fetch runs from specific experiments
    experiment_ids = ["1", "2"]
    runs_df = mlflow.search_runs(experiment_ids)
    ```

2.  **Filter and Sort**: With the data in a DataFrame, you can use pandas to sort and filter the results to find the top-performing runs based on your criteria.

    ```python
    # Find the best run based on validation accuracy
    best_run = runs_df.sort_values("metrics.validation_accuracy", ascending=False).iloc[0]
    print(f"Best Run ID: {best_run.run_id}")
    ```

3.  **Visualize Comparisons**: Use libraries like [Matplotlib](https://matplotlib.org/) or [Seaborn](https://seaborn.pydata.org/) to create custom visualizations that make comparisons clear and intuitive.

    ```python
    import matplotlib.pyplot as plt

    # Plot validation accuracy for the top 5 runs
    top_5_runs = runs_df.sort_values("metrics.validation_accuracy", ascending=False).head(5)
    plt.figure(figsize=(12, 7))
    plt.bar(top_5_runs["run_id"].str[:7], top_5_runs["metrics.validation_accuracy"])
    plt.title("Comparison of Validation Accuracy Across Top 5 Runs")
    plt.xlabel("Run ID")
    plt.ylabel("Validation Accuracy")
    plt.show()
    ```

## What are some best practices for experiment tracking?

To maximize the value of your experiments, adopt these practices:

- **Use Clear Naming Conventions**: Give experiments and runs descriptive names to make them easily identifiable (e.g., `PROD_Retraining_ResNet50` vs. `test_run_1`).
- **Align with Business Metrics**: Ensure that the metrics you track are directly relevant to project goals and business outcomes.
- **Leverage Nested Runs**: Use [nested runs](https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/part1-child-runs.html) to organize complex experiments, such as hyperparameter tuning, where each child run explores a different parameter set.
    ```python
    with mlflow.start_run(run_name="Hyperparameter Search") as parent_run:
        params = [0.01, 0.02, 0.03]
        for p in params:
            with mlflow.start_run(nested=True, run_name=f"alpha_{p}") as child_run:
                mlflow.log_param("alpha", p)
                # ... training logic ...
                mlflow.log_metric("val_loss", val_loss)
    ```
- **Tag Extensively**: [Use tags](https://mlflow.org/docs/latest/tracking/tracking-api.html#add-tags-to-runs) to add metadata like the dataset version, model type, or evaluation status (e.g., `dataset:v2`, `model:xgboost`, `status:production_candidate`).
- **Track Progress Over Time**: Log metrics at each step or epoch to visualize the learning process and diagnose issues like overfitting.
    ```python
    # Inside your training loop
    mlflow.log_metric(key="train_loss", value=train_loss, step=epoch)
    ```
- **Register Promising Models**: When a run produces a high-quality model, log it to the [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html) to version it and prepare it for deployment.

## How does experiment tracking fit into the MLOps lifecycle?

Experiment tracking is a cornerstone of the MLOps lifecycle, bridging the gap between development and production.

- **Development**: It provides the tools to systematically explore and refine models.
- **CI/CD Integration**: The artifacts and metadata from experiments feed directly into continuous integration and deployment pipelines. For example, a CI pipeline can automatically trigger when a new model is registered, running tests and preparing it for deployment.
- **Production Monitoring**: The metrics and parameters from training runs serve as a baseline for monitoring the model's performance in production. If performance degrades (a concept known as model drift), the tracked experiments provide a clear, reproducible history to inform retraining and debugging efforts.

By maintaining a detailed record of every experiment, you create a transparent, auditable, and efficient workflow that accelerates the entire MLOps cycle.

## Additional Resources

- **[AI-ML Experiment integration from the MLOps Python Package](https://github.com/fmind/mlops-python-package/blob/main/src/bikes/io/services.py)**
- [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html)
- [Experiment Tracking with MLflow in 10 Minutes](https://towardsdatascience.com/experiment-tracking-with-mlflow-in-10-minutes-f7c2128b8f2c)
- [How We Track Machine Learning Experiments with MLFlow](https://medium.com/towards-data-science/how-we-track-machine-learning-experiments-with-mlflow-948ff158a09a)
